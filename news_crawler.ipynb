{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "testenv",
   "display_name": "TestKernel",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime,timedelta,date\n",
    "import pickle, progressbar, json, glob, time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "sleep_sec = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Utility\n",
    "def date_range(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date + timedelta(1)).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "def parse_date(s):\n",
    "    return datetime.strptime(s, '%Y.%m.%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press Crawler\n",
    "press_list=['매일경제']\n",
    "def crawling_main_text(url):\n",
    "    req = requests.get(url)\n",
    "    req.encoding = None\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    try:\n",
    "        if 'mirakle.mk' in url:\n",
    "            text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "        elif 'mk.co' in url:\n",
    "            text = soup.find('div', {'class' : 'art_txt'}).text\n",
    "        else:\n",
    "            raise Exception()\n",
    "    except:\n",
    "        return None\n",
    "    return text.replace('\\n','').replace('\\r','').replace('<br>','').replace('\\t','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naver Crawler\n",
    "keyword = input('Keyword: ')\n",
    "news_num_per_day = int(input('crawl count per day: '))\n",
    "date_start = parse_date(input('start date(YYYY.MM.DD): '))\n",
    "date_end = parse_date(input('end date(YYYY.MM.DD): '))\n",
    "\n",
    "driver_path = '/usr/bin/chromedriver'\n",
    "browser = webdriver.Chrome(driver_path)\n",
    "\n",
    "news_list=[]\n",
    "for date in date_range(date_start,date_end):\n",
    "    news_url = 'https://search.naver.com/search.naver?where=news&query={0}&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={1}'.format(keyword,date.strftime('%Y.%m.%d'))\n",
    "    #headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "    browser.get(news_url)#,headers=headers)\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    # 언론사 선택\n",
    "    search_opt_box = browser.find_element_by_xpath('//*[@id=\"search_option_button\"]')\n",
    "    if search_opt_box.get_attribute('aria-pressed')=='false':\n",
    "        search_opt_box.click()\n",
    "    time.sleep(0.02)\n",
    "    tablist_box = browser.find_element_by_xpath('//div[@class=\"snb_inner\"]/ul[@role=\"tablist\" and @class=\"option_menu\"]')\n",
    "    tablist_elem_list = tablist_box.find_elements_by_xpath('./li[@role=\"presentation\"]')\n",
    "    press_box = [t for t in tablist_elem_list if t.text == '언론사'][0].find_element_by_xpath('./a')\n",
    "    press_box.click()\n",
    "    actived_press_frame = browser.find_element_by_xpath('.//div[@class=\"snb_itembox lst_press _search_option_press_\"]')\n",
    "    total_press_box = actived_press_frame.find_element_by_xpath('./div[@class=\"group_sort type_press _group_by_press_\"]')\n",
    "    press_cat_active_button = total_press_box.find_elements_by_xpath('.//a[@role=\"tab\" and @class=\"item _tab_filter_\"]')\n",
    "    press_cat_active_button_dict = dict(zip([t.text for t in press_cat_active_button], press_cat_active_button))\n",
    "    each_press_box_list = total_press_box.find_elements_by_xpath('.//div[@class=\"scroll_area _panel_filter_\"]')\n",
    "    for idx, press_cat_name in enumerate(press_cat_active_button_dict.keys()):\n",
    "        press_cat_active_button_dict[press_cat_name].click()\n",
    "        time.sleep(0.05)\n",
    "        each_press_box = each_press_box_list[idx].find_element_by_xpath('./div[@class=\"select_item\"]')\n",
    "        each_press_title_list = [ep.get_attribute('title') for ep in each_press_box.find_elements_by_xpath('.//label')]\n",
    "        each_press_input_list = each_press_box.find_elements_by_xpath('.//input')\n",
    "        each_press_title_input_dict = dict(zip(each_press_title_list, each_press_input_list))\n",
    "        for title in [tit for tit in each_press_title_input_dict.keys() if tit in press_list]:\n",
    "            each_press_title_input_dict[title].click()\n",
    "    confirm_buttons = actived_press_frame.find_element_by_xpath('./span[@class=\"btn_inp\"]').find_elements_by_xpath('.//button')\n",
    "    ok_button = [c for c in confirm_buttons if c.text == '확인'][0]\n",
    "    ok_button.click()\n",
    "    \n",
    "    print('Crawling news about {} on {}'.format(keyword,date.strftime('%Y.%m.%d')))\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    idx = 0\n",
    "    cur_page = 1\n",
    "    pbar = tqdm(total=news_num_per_day)\n",
    "    while idx < news_num_per_day:\n",
    "        table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "        li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n",
    "        area_list = [li.find_element_by_xpath('.//div[@class=\"news_area\"]') for li in li_list]\n",
    "        a_list = [area.find_element_by_xpath('.//a[@class=\"news_tit\"]') for area in area_list]\n",
    "\n",
    "        for n in a_list[:min(len(a_list), news_num_per_day-idx)]:\n",
    "            n_url = n.get_attribute('href')\n",
    "            text = crawling_main_text(n_url)\n",
    "            if text:\n",
    "                news_list.append({'title':n.get_attribute('title'),'url':n_url,'text':text})\n",
    "                idx += 1\n",
    "                pbar.update(1)\n",
    "        \n",
    "        if idx < news_num_per_day:\n",
    "            cur_page +=1\n",
    "            pages = browser.find_element_by_xpath('//div[@class=\"sc_page_inner\"]')\n",
    "            next_page = [p for p in pages.find_elements_by_xpath('.//a') if p.text == str(cur_page)]\n",
    "            if not next_page:\n",
    "                break\n",
    "            next_page_url = next_page[0].get_attribute('href')\n",
    "            browser.get(next_page_url)\n",
    "            time.sleep(sleep_sec)\n",
    "    pbar.close()\n",
    "browser.close()\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "news_df = DataFrame(dict(enumerate(news_list))).T\n",
    "folder_path = os.getcwd()\n",
    "file_name = '{}_{}_{}_{}.csv'.format(keyword,date_start.strftime('%Y.%m.%d'),date_end.strftime('%Y.%m.%d'),news_num_per_day)\n",
    "news_df.to_csv(file_name)\n",
    "print('Saved at {}\\\\{}'.format(folder_path,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_list[1])\n",
    "print(news_list[4])\n"
   ]
  }
 ]
}