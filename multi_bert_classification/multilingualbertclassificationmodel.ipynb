{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install keras\n!pip install tensorflow\n!pip install jupyter-resource-usage\n!pip install mxnet\n!pip install gluonnlp pandas tqdm\n!pip install sentencepiece\n!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T05:20:44.958344Z","iopub.execute_input":"2021-05-26T05:20:44.958627Z","iopub.status.idle":"2021-05-26T05:21:49.624472Z","shell.execute_reply.started":"2021-05-26T05:20:44.958561Z","shell.execute_reply":"2021-05-26T05:21:49.623685Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.4.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.56.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.11.13)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.3)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (2.4.3)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras) (2.10.0)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras) (1.5.4)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras) (1.19.5)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras) (5.3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras) (1.15.0)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.4.1)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.36.2)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.12.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.19.5)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.4.1)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.32.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.15.6)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.3.3)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.24.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (49.6.0.post20210108)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.1)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.3)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\nCollecting jupyter-resource-usage\n  Downloading jupyter_resource_usage-0.6.0-py2.py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 41 kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from jupyter-resource-usage) (0.9.0)\nCollecting jupyter-server>=1.0.0\n  Downloading jupyter_server-1.8.0-py3-none-any.whl (382 kB)\n\u001b[K     |████████████████████████████████| 382 kB 465 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: psutil>=5.6.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-resource-usage) (5.8.0)\nRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (0.2.0)\nCollecting tornado>=6.1.0\n  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n\u001b[K     |████████████████████████████████| 428 kB 4.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (6.0.7)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (5.1.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (0.9.2)\nCollecting anyio<4,>=3.1.0\n  Downloading anyio-3.1.0-py3-none-any.whl (74 kB)\n\u001b[K     |████████████████████████████████| 74 kB 2.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: traitlets>=4.2.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (5.0.5)\nRequirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (4.7.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (2.11.3)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (0.57.0)\nRequirement already satisfied: jupyter-client>=6.1.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (6.1.11)\nRequirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (22.0.3)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (20.1.0)\nRequirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (1.5.0)\nCollecting sniffio>=1.1\n  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.7/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.0.0->jupyter-resource-usage) (2.10)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.0.0->jupyter-resource-usage) (3.7.4.3)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.1->jupyter-server>=1.0.0->jupyter-resource-usage) (2.8.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.1->jupyter-server>=1.0.0->jupyter-resource-usage) (1.15.0)\nRequirement already satisfied: ptyprocess in /opt/conda/lib/python3.7/site-packages (from terminado>=0.8.3->jupyter-server>=1.0.0->jupyter-resource-usage) (0.7.0)\nRequirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->jupyter-server>=1.0.0->jupyter-resource-usage) (1.14.5)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server>=1.0.0->jupyter-resource-usage) (2.20)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->jupyter-server>=1.0.0->jupyter-resource-usage) (1.1.1)\nRequirement already satisfied: pygments>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (2.8.0)\nRequirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.3)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.5.2)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (1.4.2)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.1.2)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (3.3.0)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.8.4)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.6.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.4.4)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (1.4.3)\nRequirement already satisfied: async-generator in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (1.10)\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (3.2.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (49.6.0.post20210108)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (3.4.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (20.3.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (0.17.3)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.5.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (20.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->bleach->nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (2.4.7)\nInstalling collected packages: tornado, sniffio, anyio, jupyter-server, jupyter-resource-usage\n  Attempting uninstall: tornado\n    Found existing installation: tornado 5.0.2\n    Uninstalling tornado-5.0.2:\n      Successfully uninstalled tornado-5.0.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab-git 0.11.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 2.1.0 which is incompatible.\u001b[0m\nSuccessfully installed anyio-3.1.0 jupyter-resource-usage-0.6.0 jupyter-server-1.8.0 sniffio-1.2.0 tornado-6.1\nCollecting mxnet\n  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n\u001b[K     |████████████████████████████████| 46.9 MB 191 kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (1.19.5)\nRequirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (2.25.1)\nRequirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from mxnet) (0.8.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.3)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\nInstalling collected packages: mxnet\nSuccessfully installed mxnet-1.8.0.post0\nRequirement already satisfied: gluonnlp in /opt/conda/lib/python3.7/site-packages (0.10.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.1.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.56.2)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (1.19.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (20.9)\nRequirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (0.29.22)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->gluonnlp) (2.4.7)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\nCollecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-k85e4dww\n  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-k85e4dww\nBuilding wheels for collected packages: kobert\n  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=12704 sha256=d68e4622f9f8341f1b38d0a2b1626dc07d850b7a0879e8739ae645aa2ef975c1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-y9u74i62/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\nSuccessfully built kobert\nInstalling collected packages: kobert\nSuccessfully installed kobert-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# !wget https://raw.githubusercontent.com/yunakim2/OpenSourceProject/feat/bertModel/data/test_data/all_processing_01_eng.csv\n!wget https://raw.githubusercontent.com/yunakim2/OpenSourceProject/feat/bertModel/data/test_data/all_processing_01.csv\n\nimport gc\nimport torch\n\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:42:47.759023Z","iopub.execute_input":"2021-05-26T05:42:47.759354Z","iopub.status.idle":"2021-05-26T05:42:51.166246Z","shell.execute_reply.started":"2021-05-26T05:42:47.759325Z","shell.execute_reply":"2021-05-26T05:42:51.165276Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"--2021-05-26 05:42:48--  https://raw.githubusercontent.com/yunakim2/OpenSourceProject/feat/bertModel/data/test_data/all_processing_01.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 20423172 (19M) [text/plain]\nSaving to: ‘all_processing_01.csv’\n\nall_processing_01.c 100%[===================>]  19.48M  22.1MB/s    in 0.9s    \n\n2021-05-26 05:42:51 (22.1 MB/s) - ‘all_processing_01.csv’ saved [20423172/20423172]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nn_devices = torch.cuda.device_count()\nprint(n_devices)\n\nfor i in range(n_devices):\n    print(torch.cuda.get_device_name(i))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:42:54.159572Z","iopub.execute_input":"2021-05-26T05:42:54.159917Z","iopub.status.idle":"2021-05-26T05:42:54.168403Z","shell.execute_reply.started":"2021-05-26T05:42:54.159885Z","shell.execute_reply":"2021-05-26T05:42:54.167315Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1\nTesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('all_processing_01.csv',encoding = 'utf-8')\ntest_cnt = int(data.shape[0] * 0.25)\n\ntest = data[:test_cnt]\ntrain = data[test_cnt:]\ndocument_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in train['text']]\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n\ntokenized_texts = [tokenizer.tokenize(s) for s in document_bert]\n\nMAX_LEN = 128\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\nattention_masks = []\n\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)\n    \n\ntrain_inputs, validation_inputs, train_labels, validation_labels = \\\ntrain_test_split(input_ids, train['label'].values, random_state=42, test_size=0.1)\n\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, \n                                                       input_ids,\n                                                       random_state=42, \n                                                       test_size=0.1)\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:42:56.744310Z","iopub.execute_input":"2021-05-26T05:42:56.744620Z","iopub.status.idle":"2021-05-26T05:43:41.576655Z","shell.execute_reply.started":"2021-05-26T05:42:56.744591Z","shell.execute_reply":"2021-05-26T05:43:41.575867Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"752a7fe80aff4aadad292e69600999bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2776a6248ba4de793ad7d53edea0816"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ca251ba17cf4979ac01bbb0ad130a14"}},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:43:41.578397Z","iopub.execute_input":"2021-05-26T05:43:41.578710Z","iopub.status.idle":"2021-05-26T05:43:41.584366Z","shell.execute_reply.started":"2021-05-26T05:43:41.578676Z","shell.execute_reply":"2021-05-26T05:43:41.583525Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"sentences = test['text']\nsentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\nlabels = test['label'].values\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\nattention_masks = []\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)\n\ntest_inputs = torch.tensor(input_ids)\ntest_labels = torch.tensor(labels)\ntest_masks = torch.tensor(attention_masks)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:43:41.585694Z","iopub.execute_input":"2021-05-26T05:43:41.586281Z","iopub.status.idle":"2021-05-26T05:43:52.594927Z","shell.execute_reply.started":"2021-05-26T05:43:41.586243Z","shell.execute_reply":"2021-05-26T05:43:52.594138Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\nmodel.cuda()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:43:52.596185Z","iopub.execute_input":"2021-05-26T05:43:52.596505Z","iopub.status.idle":"2021-05-26T05:45:03.889613Z","shell.execute_reply.started":"2021-05-26T05:43:52.596472Z","shell.execute_reply":"2021-05-26T05:45:03.888866Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"679d3e5d8b3c414b80c9aa5c16bdd1e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c60498bef040c4bbf4b98cd56f4137"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# 옵티마이저 설정\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # 학습률\n                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n                )\n\n# 에폭수\nepochs = 4\n\n# 총 훈련 스텝\ntotal_steps = len(train_dataloader) * epochs\n\n# lr 조금씩 감소시키는 스케줄러\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:45:03.892167Z","iopub.execute_input":"2021-05-26T05:45:03.892515Z","iopub.status.idle":"2021-05-26T05:45:03.901394Z","shell.execute_reply.started":"2021-05-26T05:45:03.892479Z","shell.execute_reply":"2021-05-26T05:45:03.900662Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# 정확도 계산 함수\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# 시간 표시 함수\ndef format_time(elapsed):\n    # 반올림\n    elapsed_rounded = int(round((elapsed)))\n    # hh:mm:ss으로 형태 변경\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:45:03.902811Z","iopub.execute_input":"2021-05-26T05:45:03.903291Z","iopub.status.idle":"2021-05-26T05:45:04.168311Z","shell.execute_reply.started":"2021-05-26T05:45:03.903258Z","shell.execute_reply":"2021-05-26T05:45:04.167413Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# 재현을 위해 랜덤시드 고정\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# 그래디언트 초기화\nmodel.zero_grad()\ndevice = \"cuda:0\"\nmodel = model.to(device)\n# 에폭만큼 반복\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # 시작 시간 설정\n    t0 = time.time()\n\n    # 로스 초기화\n    total_loss = 0\n\n    # 훈련모드로 변경\n    model.train()\n        \n    # 데이터로더에서 배치만큼 반복하여 가져옴\n    for step, batch in enumerate(train_dataloader):\n        # 경과 정보 표시\n        if step % 500 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # 배치를 GPU에 넣음\n        batch = tuple(t.to(device) for t in batch)\n        \n        # 배치에서 데이터 추출\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Forward 수행                \n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n        \n        # 로스 구함\n        loss = outputs[0]\n\n        # 총 로스 계산\n        total_loss += loss.item()\n\n        # Backward 수행으로 그래디언트 계산\n        loss.backward()\n\n        # 그래디언트 클리핑\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # 그래디언트를 통해 가중치 파라미터 업데이트\n        optimizer.step()\n\n        # 스케줄러로 학습률 감소\n        scheduler.step()\n\n        # 그래디언트 초기화\n        model.zero_grad()\n\n    # 평균 로스 계산\n    avg_train_loss = total_loss / len(train_dataloader)            \n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    #시작 시간 설정\n    t0 = time.time()\n\n    # 평가모드로 변경\n    model.eval()\n\n    # 변수 초기화\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # 데이터로더에서 배치만큼 반복하여 가져옴\n    for batch in validation_dataloader:\n        # 배치를 GPU에 넣음\n        batch = tuple(t.to(device) for t in batch)\n        \n        # 배치에서 데이터 추출\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # 그래디언트 계산 안함\n        with torch.no_grad():     \n            # Forward 수행\n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # 로스 구함\n        logits = outputs[0]\n\n        # CPU로 데이터 이동\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # 출력 로짓과 라벨을 비교하여 정확도 계산\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n\nprint(\"\")\nprint(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:45:04.172405Z","iopub.execute_input":"2021-05-26T05:45:04.172719Z","iopub.status.idle":"2021-05-26T05:49:16.217451Z","shell.execute_reply.started":"2021-05-26T05:45:04.172691Z","shell.execute_reply":"2021-05-26T05:49:16.216459Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 4 ========\nTraining...\n\n  Average training loss: 0.70\n  Training epcoh took: 0:01:01\n\nRunning Validation...\n  Accuracy: 0.53\n  Validation took: 0:00:02\n\n======== Epoch 2 / 4 ========\nTraining...\n\n  Average training loss: 0.69\n  Training epcoh took: 0:01:01\n\nRunning Validation...\n  Accuracy: 0.53\n  Validation took: 0:00:02\n\n======== Epoch 3 / 4 ========\nTraining...\n\n  Average training loss: 0.69\n  Training epcoh took: 0:01:01\n\nRunning Validation...\n  Accuracy: 0.53\n  Validation took: 0:00:02\n\n======== Epoch 4 / 4 ========\nTraining...\n\n  Average training loss: 0.69\n  Training epcoh took: 0:01:01\n\nRunning Validation...\n  Accuracy: 0.53\n  Validation took: 0:00:02\n\nTraining complete!\n","output_type":"stream"}]},{"cell_type":"code","source":"#시작 시간 설정\nt0 = time.time()\n\n# 평가모드로 변경\nmodel.eval()\n\n\n# 변수 초기화\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\n# 데이터로더에서 배치만큼 반복하여 가져옴\nfor step, batch in enumerate(test_dataloader):\n    # 경과 정보 표시\n    if step % 100 == 0 and not step == 0:\n        elapsed = format_time(time.time() - t0)\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n\n    # 배치를 GPU에 넣음\n    batch = tuple(t.to(device) for t in batch)\n    \n    # 배치에서 데이터 추출\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    # 그래디언트 계산 안함\n    with torch.no_grad():     \n        # Forward 수행\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n    \n    # 로스 구함\n    logits = outputs[0]\n\n    # CPU로 데이터 이동\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    # 출력 로짓과 라벨을 비교하여 정확도 계산\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\nprint(\"\")\nprint(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\nprint(\"Test took: {:}\".format(format_time(time.time() - t0)))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:49:16.220574Z","iopub.execute_input":"2021-05-26T05:49:16.220860Z","iopub.status.idle":"2021-05-26T05:49:23.164144Z","shell.execute_reply.started":"2021-05-26T05:49:16.220821Z","shell.execute_reply":"2021-05-26T05:49:23.163258Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"\nAccuracy: 0.53\nTest took: 0:00:07\n","output_type":"stream"}]}]}