{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install keras\n!pip install tensorflow\n!pip install jupyter-resource-usage\n!pip install mxnet\n!pip install gluonnlp pandas tqdm\n!pip install sentencepiece\n!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T05:20:44.958344Z","iopub.execute_input":"2021-05-26T05:20:44.958627Z","iopub.status.idle":"2021-05-26T05:21:49.624472Z","shell.execute_reply.started":"2021-05-26T05:20:44.958561Z","shell.execute_reply":"2021-05-26T05:21:49.623685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !wget https://raw.githubusercontent.com/yunakim2/OpenSourceProject/feat/bertModel/data/test_data/all_processing_01_eng.csv\n!wget https://raw.githubusercontent.com/yunakim2/OpenSourceProject/feat/bertModel/data/test_data/all_processing_01.csv\n\nimport gc\nimport torch\n\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:42:47.759023Z","iopub.execute_input":"2021-05-26T05:42:47.759354Z","iopub.status.idle":"2021-05-26T05:42:51.166246Z","shell.execute_reply.started":"2021-05-26T05:42:47.759325Z","shell.execute_reply":"2021-05-26T05:42:51.165276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nn_devices = torch.cuda.device_count()\nprint(n_devices)\n\nfor i in range(n_devices):\n    print(torch.cuda.get_device_name(i))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:42:54.159572Z","iopub.execute_input":"2021-05-26T05:42:54.159917Z","iopub.status.idle":"2021-05-26T05:42:54.168403Z","shell.execute_reply.started":"2021-05-26T05:42:54.159885Z","shell.execute_reply":"2021-05-26T05:42:54.167315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('all_processing_01.csv',encoding = 'utf-8')\ntest_cnt = int(data.shape[0] * 0.25)\n\ntest = data[:test_cnt]\ntrain = data[test_cnt:]\ndocument_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in train['text']]\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n\ntokenized_texts = [tokenizer.tokenize(s) for s in document_bert]\n\nMAX_LEN = 128\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\nattention_masks = []\n\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)\n    \n\ntrain_inputs, validation_inputs, train_labels, validation_labels = \\\ntrain_test_split(input_ids, train['label'].values, random_state=42, test_size=0.1)\n\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, \n                                                       input_ids,\n                                                       random_state=42, \n                                                       test_size=0.1)\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:42:56.74431Z","iopub.execute_input":"2021-05-26T05:42:56.74462Z","iopub.status.idle":"2021-05-26T05:43:41.576655Z","shell.execute_reply.started":"2021-05-26T05:42:56.744591Z","shell.execute_reply":"2021-05-26T05:43:41.575867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:43:41.578397Z","iopub.execute_input":"2021-05-26T05:43:41.57871Z","iopub.status.idle":"2021-05-26T05:43:41.584366Z","shell.execute_reply.started":"2021-05-26T05:43:41.578676Z","shell.execute_reply":"2021-05-26T05:43:41.583525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = test['text']\nsentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\nlabels = test['label'].values\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\nattention_masks = []\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)\n\ntest_inputs = torch.tensor(input_ids)\ntest_labels = torch.tensor(labels)\ntest_masks = torch.tensor(attention_masks)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:43:41.585694Z","iopub.execute_input":"2021-05-26T05:43:41.586281Z","iopub.status.idle":"2021-05-26T05:43:52.594927Z","shell.execute_reply.started":"2021-05-26T05:43:41.586243Z","shell.execute_reply":"2021-05-26T05:43:52.594138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\nmodel.cuda()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:43:52.596185Z","iopub.execute_input":"2021-05-26T05:43:52.596505Z","iopub.status.idle":"2021-05-26T05:45:03.889613Z","shell.execute_reply.started":"2021-05-26T05:43:52.596472Z","shell.execute_reply":"2021-05-26T05:45:03.888866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 옵티마이저 설정\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # 학습률\n                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n                )\n\n# 에폭수\nepochs = 4\n\n# 총 훈련 스텝\ntotal_steps = len(train_dataloader) * epochs\n\n# lr 조금씩 감소시키는 스케줄러\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:45:03.892167Z","iopub.execute_input":"2021-05-26T05:45:03.892515Z","iopub.status.idle":"2021-05-26T05:45:03.901394Z","shell.execute_reply.started":"2021-05-26T05:45:03.892479Z","shell.execute_reply":"2021-05-26T05:45:03.900662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 정확도 계산 함수\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# 시간 표시 함수\ndef format_time(elapsed):\n    # 반올림\n    elapsed_rounded = int(round((elapsed)))\n    # hh:mm:ss으로 형태 변경\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:45:03.902811Z","iopub.execute_input":"2021-05-26T05:45:03.903291Z","iopub.status.idle":"2021-05-26T05:45:04.168311Z","shell.execute_reply.started":"2021-05-26T05:45:03.903258Z","shell.execute_reply":"2021-05-26T05:45:04.167413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 재현을 위해 랜덤시드 고정\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# 그래디언트 초기화\nmodel.zero_grad()\ndevice = \"cuda:0\"\nmodel = model.to(device)\n# 에폭만큼 반복\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # 시작 시간 설정\n    t0 = time.time()\n\n    # 로스 초기화\n    total_loss = 0\n\n    # 훈련모드로 변경\n    model.train()\n        \n    # 데이터로더에서 배치만큼 반복하여 가져옴\n    for step, batch in enumerate(train_dataloader):\n        # 경과 정보 표시\n        if step % 500 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # 배치를 GPU에 넣음\n        batch = tuple(t.to(device) for t in batch)\n        \n        # 배치에서 데이터 추출\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Forward 수행                \n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n        \n        # 로스 구함\n        loss = outputs[0]\n\n        # 총 로스 계산\n        total_loss += loss.item()\n\n        # Backward 수행으로 그래디언트 계산\n        loss.backward()\n\n        # 그래디언트 클리핑\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # 그래디언트를 통해 가중치 파라미터 업데이트\n        optimizer.step()\n\n        # 스케줄러로 학습률 감소\n        scheduler.step()\n\n        # 그래디언트 초기화\n        model.zero_grad()\n\n    # 평균 로스 계산\n    avg_train_loss = total_loss / len(train_dataloader)            \n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    #시작 시간 설정\n    t0 = time.time()\n\n    # 평가모드로 변경\n    model.eval()\n\n    # 변수 초기화\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # 데이터로더에서 배치만큼 반복하여 가져옴\n    for batch in validation_dataloader:\n        # 배치를 GPU에 넣음\n        batch = tuple(t.to(device) for t in batch)\n        \n        # 배치에서 데이터 추출\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # 그래디언트 계산 안함\n        with torch.no_grad():     \n            # Forward 수행\n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # 로스 구함\n        logits = outputs[0]\n\n        # CPU로 데이터 이동\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # 출력 로짓과 라벨을 비교하여 정확도 계산\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n\nprint(\"\")\nprint(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:45:04.172405Z","iopub.execute_input":"2021-05-26T05:45:04.172719Z","iopub.status.idle":"2021-05-26T05:49:16.217451Z","shell.execute_reply.started":"2021-05-26T05:45:04.172691Z","shell.execute_reply":"2021-05-26T05:49:16.216459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#시작 시간 설정\nt0 = time.time()\n\n# 평가모드로 변경\nmodel.eval()\n\n\n# 변수 초기화\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\n# 데이터로더에서 배치만큼 반복하여 가져옴\nfor step, batch in enumerate(test_dataloader):\n    # 경과 정보 표시\n    if step % 100 == 0 and not step == 0:\n        elapsed = format_time(time.time() - t0)\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n\n    # 배치를 GPU에 넣음\n    batch = tuple(t.to(device) for t in batch)\n    \n    # 배치에서 데이터 추출\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    # 그래디언트 계산 안함\n    with torch.no_grad():     \n        # Forward 수행\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n    \n    # 로스 구함\n    logits = outputs[0]\n\n    # CPU로 데이터 이동\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    # 출력 로짓과 라벨을 비교하여 정확도 계산\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\nprint(\"\")\nprint(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\nprint(\"Test took: {:}\".format(format_time(time.time() - t0)))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:49:16.220574Z","iopub.execute_input":"2021-05-26T05:49:16.22086Z","iopub.status.idle":"2021-05-26T05:49:23.164144Z","shell.execute_reply.started":"2021-05-26T05:49:16.220821Z","shell.execute_reply":"2021-05-26T05:49:23.163258Z"},"trusted":true},"execution_count":null,"outputs":[]}]}