{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ccf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install jupyter-resource-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0db8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# In[]\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "USE_CUDA = True\n",
    "RANDOM_SEED=43 # 재현을 위해 랜덤시드 고정\n",
    "TOKEN_MAX_LEN = 128*4\n",
    "BATCH_SIZE = 12\n",
    "STATUS_PRINT_INTERVAL=25\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "\n",
    "if USE_CUDA and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53244254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train&validation data processing\n",
      "test data processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "   25/  695, Elapsed 0:00:30\n",
      "   50/  695, Elapsed 0:01:01\n",
      "   75/  695, Elapsed 0:01:32\n",
      "  100/  695, Elapsed 0:02:02\n",
      "  125/  695, Elapsed 0:02:33\n",
      "  150/  695, Elapsed 0:03:03\n",
      "  175/  695, Elapsed 0:03:33\n",
      "  200/  695, Elapsed 0:04:04\n",
      "  225/  695, Elapsed 0:04:34\n",
      "  250/  695, Elapsed 0:05:05\n",
      "  275/  695, Elapsed 0:05:35\n",
      "  300/  695, Elapsed 0:06:05\n",
      "  325/  695, Elapsed 0:06:35\n",
      "  350/  695, Elapsed 0:07:06\n",
      "  375/  695, Elapsed 0:07:36\n",
      "  400/  695, Elapsed 0:08:07\n",
      "  425/  695, Elapsed 0:08:37\n",
      "  450/  695, Elapsed 0:09:07\n",
      "  475/  695, Elapsed 0:09:38\n",
      "  500/  695, Elapsed 0:10:09\n",
      "  525/  695, Elapsed 0:10:39\n",
      "  550/  695, Elapsed 0:11:09\n",
      "  575/  695, Elapsed 0:11:39\n",
      "  600/  695, Elapsed 0:12:09\n",
      "  625/  695, Elapsed 0:12:40\n",
      "  650/  695, Elapsed 0:13:10\n",
      "  675/  695, Elapsed 0:13:41\n",
      "\n",
      "  Average training loss: 0.01001529\n",
      "  Training epcoh took: 0:14:04\n",
      "\n",
      "Running Validation...\n",
      "  Validation MAE: 0.01536852\n",
      "  Validation took: 0:00:34\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "   25/  695, Elapsed 0:00:31\n",
      "   50/  695, Elapsed 0:01:01\n",
      "   75/  695, Elapsed 0:01:31\n",
      "  100/  695, Elapsed 0:02:02\n",
      "  125/  695, Elapsed 0:02:33\n",
      "  150/  695, Elapsed 0:03:03\n",
      "  175/  695, Elapsed 0:03:33\n",
      "  200/  695, Elapsed 0:04:03\n",
      "  225/  695, Elapsed 0:04:33\n",
      "  250/  695, Elapsed 0:05:04\n",
      "  275/  695, Elapsed 0:05:34\n",
      "  300/  695, Elapsed 0:06:05\n",
      "  325/  695, Elapsed 0:06:35\n",
      "  350/  695, Elapsed 0:07:05\n",
      "  375/  695, Elapsed 0:07:36\n",
      "  400/  695, Elapsed 0:08:06\n",
      "  425/  695, Elapsed 0:08:36\n",
      "  450/  695, Elapsed 0:09:07\n",
      "  475/  695, Elapsed 0:09:37\n",
      "  500/  695, Elapsed 0:10:07\n",
      "  525/  695, Elapsed 0:10:37\n",
      "  550/  695, Elapsed 0:11:07\n",
      "  575/  695, Elapsed 0:11:38\n",
      "  600/  695, Elapsed 0:12:08\n",
      "  625/  695, Elapsed 0:12:39\n",
      "  650/  695, Elapsed 0:13:09\n",
      "  675/  695, Elapsed 0:13:39\n",
      "\n",
      "  Average training loss: 0.00982447\n",
      "  Training epcoh took: 0:14:03\n",
      "\n",
      "Running Validation...\n",
      "  Validation MAE: 0.01205335\n",
      "  Validation took: 0:00:34\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "   25/  695, Elapsed 0:00:31\n",
      "   50/  695, Elapsed 0:01:01\n",
      "   75/  695, Elapsed 0:01:32\n",
      "  100/  695, Elapsed 0:02:02\n",
      "  125/  695, Elapsed 0:02:32\n",
      "  150/  695, Elapsed 0:03:03\n",
      "  175/  695, Elapsed 0:03:33\n",
      "  200/  695, Elapsed 0:04:03\n",
      "  225/  695, Elapsed 0:04:34\n",
      "  250/  695, Elapsed 0:05:04\n",
      "  275/  695, Elapsed 0:05:34\n",
      "  300/  695, Elapsed 0:06:04\n",
      "  325/  695, Elapsed 0:06:34\n",
      "  350/  695, Elapsed 0:07:05\n",
      "  375/  695, Elapsed 0:07:35\n",
      "  400/  695, Elapsed 0:08:06\n",
      "  425/  695, Elapsed 0:08:36\n",
      "  450/  695, Elapsed 0:09:07\n",
      "  475/  695, Elapsed 0:09:37\n",
      "  500/  695, Elapsed 0:10:07\n",
      "  525/  695, Elapsed 0:10:37\n",
      "  550/  695, Elapsed 0:11:08\n",
      "  575/  695, Elapsed 0:11:38\n",
      "  600/  695, Elapsed 0:12:08\n",
      "  625/  695, Elapsed 0:12:39\n",
      "  650/  695, Elapsed 0:13:09\n",
      "  675/  695, Elapsed 0:13:40\n",
      "\n",
      "  Average training loss: 0.00981288\n",
      "  Training epcoh took: 0:14:03\n",
      "\n",
      "Running Validation...\n",
      "  Validation MAE: 0.01138396\n",
      "  Validation took: 0:00:34\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "   25/  695, Elapsed 0:00:30\n",
      "   50/  695, Elapsed 0:01:01\n",
      "   75/  695, Elapsed 0:01:31\n",
      "  100/  695, Elapsed 0:02:01\n",
      "  125/  695, Elapsed 0:02:31\n",
      "  150/  695, Elapsed 0:03:02\n",
      "  175/  695, Elapsed 0:03:33\n",
      "  200/  695, Elapsed 0:04:03\n",
      "  225/  695, Elapsed 0:04:33\n",
      "  250/  695, Elapsed 0:05:03\n",
      "  275/  695, Elapsed 0:05:34\n",
      "  300/  695, Elapsed 0:06:04\n",
      "  325/  695, Elapsed 0:06:35\n",
      "  350/  695, Elapsed 0:07:05\n",
      "  375/  695, Elapsed 0:07:35\n",
      "  400/  695, Elapsed 0:08:06\n",
      "  425/  695, Elapsed 0:08:36\n",
      "  450/  695, Elapsed 0:09:06\n",
      "  475/  695, Elapsed 0:09:37\n",
      "  500/  695, Elapsed 0:10:07\n",
      "  525/  695, Elapsed 0:10:38\n",
      "  550/  695, Elapsed 0:11:08\n",
      "  575/  695, Elapsed 0:11:39\n",
      "  600/  695, Elapsed 0:12:09\n",
      "  625/  695, Elapsed 0:12:39\n",
      "  650/  695, Elapsed 0:13:09\n",
      "  675/  695, Elapsed 0:13:39\n",
      "\n",
      "  Average training loss: 0.00979102\n",
      "  Training epcoh took: 0:14:03\n",
      "\n",
      "Running Validation...\n",
      "  Validation MAE: 0.01277083\n",
      "  Validation took: 0:00:34\n",
      "\n",
      "Training complete!\n",
      "   25/  258, Elapsed 0:00:11\n",
      "   50/  258, Elapsed 0:00:22\n",
      "   75/  258, Elapsed 0:00:33\n",
      "  100/  258, Elapsed 0:00:44\n",
      "  125/  258, Elapsed 0:00:55\n",
      "  150/  258, Elapsed 0:01:06\n",
      "  175/  258, Elapsed 0:01:17\n",
      "  200/  258, Elapsed 0:01:28\n",
      "  225/  258, Elapsed 0:01:39\n",
      "  250/  258, Elapsed 0:01:50\n",
      "\n",
      "Test MAE: 0.01393727\n",
      "Test took: 0:01:53\n",
      "TestSet AAD: 0       0.016069\n",
      "1       0.016069\n",
      "2       0.016069\n",
      "3       0.016069\n",
      "4       0.019592\n",
      "          ...   \n",
      "3080    0.009324\n",
      "3081    0.009324\n",
      "3082    0.009324\n",
      "3083    0.009324\n",
      "3084    0.009324\n",
      "Name: label, Length: 3085, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "data = pd.read_csv('data/labeled/samsung_2010_2021.csv', encoding='utf-8', dtype={'label':np.float32})\n",
    "test_cnt = int(data.shape[0] * 0.25)\n",
    "\n",
    "test = data[:test_cnt]\n",
    "train = data[test_cnt:]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "print('train&validation data processing')\n",
    "# Train,Validation Data Preprocessing\n",
    "input_ids = [tokenizer.encode(s,max_length=TOKEN_MAX_LEN,truncation=True) for s in train['text']]\n",
    "input_ids = pad_sequences(input_ids, maxlen=TOKEN_MAX_LEN, dtype='long', truncating='post', padding='post')\n",
    "attention_mask = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_mask.append(seq_mask)\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, train['label'].values, random_state=RANDOM_SEED, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_mask,input_ids,random_state=RANDOM_SEED,test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('test data processing')\n",
    "# Test Data Preprocessing\n",
    "input_ids = [tokenizer.encode(sent,max_length=TOKEN_MAX_LEN,truncation=True) for sent in test['text']]\n",
    "input_ids = pad_sequences(input_ids, maxlen=TOKEN_MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(test['label'].values)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=1).to(device)\n",
    "print('Model Created')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        pass\n",
    "        if step and step % STATUS_PRINT_INTERVAL == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('{:>5,}/{:>5,}, Elapsed {:}'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        pred = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        loss = torch.mean(torch.abs(pred-b_labels))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(\"\\n  Average training loss: {0:.8f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\\nRunning Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    eval_mae=0\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            pred = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask)[0]\n",
    "        loss = torch.mean(torch.abs(pred-b_labels))\n",
    "        eval_mae += loss.item()\n",
    "    print(\"  Validation MAE: {0:.8f}\".format(eval_mae / len(validation_dataloader)))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "eval_mae=0\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    if step and step % STATUS_PRINT_INTERVAL == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('{:>5,}/{:>5,}, Elapsed {:}'.format(step, len(test_dataloader), elapsed))\n",
    "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        pred = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask)[0]\n",
    "    loss = torch.mean(torch.abs(pred-b_labels))\n",
    "    eval_mae += loss.item()\n",
    "print(\"\\nTest MAE: {0:.8f}\".format(eval_mae / len(test_dataloader)))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54d8d481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet AAD: 0.01192370243370533\n",
      "Label: tensor([-0.0001])\n",
      "Pred: tensor([[-0.0300]], device='cuda:0')\n",
      "Loss: 0.029930904507637024\n",
      "[CLS] 원본보기 ▲ 삼성전자 2010년형 노트PC CES서 공개삼성전자가 글로벌 노트PC 시장 공략을 본격화하고 있다. 삼성전자는 8일 미국 라스베이거스에서 열리고 있는 세계 최대 가전전시회 CES 2010에서 2010년형 프리미엄 노트북 신제품 6종을 공개한다고 밝혔다. 삼성전자는 우선 크리스털 원석과 같은 감성적인 스타일의 R780, R580, R480 제품을 선보였다. 이들 제품은 블랙 - 레드 그라데이션 컬러에 감각적인 [UNK] S [UNK] 자 모양의 배면 패턴을 적용, 빛의 각도에 따라 보다 깊이가 있는 S패턴을 연출한다. 아울러 은은하게 빛나는 발광다이오드 ( LED ) 터치패드 터치 라이팅 기술을 적용했으며 부드러운 터치감과 작업 효율이 높은 프리미엄 아일랜드 키보드 ( Island Keyboard ) 를 장착했다. 성능도 기존 플랫폼 대비 20 % 이상 속도가 빨라진 인텔의 2010년형 새 플랫폼인 [UNK] 칼펠라 ( Calpella ) [UNK] 프로세서를 탑재했으며 최대 4GB의 DDR3 1066MHZ 메모리를 내장해 한층 빠르고 강력해진 시스템 성능을 제공한다. 또한 엔디비아 최신 그래픽 ( GT 330M, 1GB gDDR3 ) 카드를 탑재해 3D게임의 다이나믹한 영상을 즐길 수 있다. 감각적인 디자인의 보급형 노트PC 라인업인 R730, R530, R430도 선보였다. 이들 제품은 크리스털 느낌의 다양한 패턴이 어우러진 세련된 디자인에 최신 기능을 두루 탑재한 것이 가장 큰 특징이다. 성면에서는 엔디비아 최신 그래픽 카드 ( 310M, 512MB gDDR3 ) 를 탑재했으며 HD LED 디스플레이를 적용해 밝고 선명한 HD 영상을 감상할 수 있다. 삼성전자 IT솔루션사업부장 남성우 부사장은 [UNK] 이번에 선보 [SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=1).to(device)\n",
    "print('TestSet AAD: {}'.format(np.mean(np.abs(np.median(test['label'])-test['label']))))\n",
    "idx=12\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(test_inputs[idx:idx+1].to(device),token_type_ids=None,attention_mask=test_masks[idx:idx+1].to(device))[0]\n",
    "    loss = torch.mean(torch.abs(pred-test_labels[idx:idx+1].to(device)))\n",
    "    print('Label: {}'.format(test_labels[idx:idx+1]))\n",
    "    print('Pred: {}'.format(pred))\n",
    "    print('Loss: {}'.format(loss))\n",
    "    print(tokenizer.decode(test_inputs[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767137c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
