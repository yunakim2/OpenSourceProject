{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install keras\n!pip install tensorflow\n!pip install jupyter-resource-usage\n!pip install mxnet\n!pip install gluonnlp pandas tqdm\n!pip install sentencepiece\n!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.5.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.3.17)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.59.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (2.4.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras) (5.3.1)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras) (1.5.4)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras) (1.19.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras) (2.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras) (1.15.0)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.4.1)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.3.3)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.32.0)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.4.1)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.12.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.15.8)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.36.2)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.19.5)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.26.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (49.6.0.post20210108)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.4)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\nCollecting jupyter-resource-usage\n  Downloading jupyter_resource_usage-0.6.0-py2.py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 241 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: psutil>=5.6.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-resource-usage) (5.8.0)\nCollecting jupyter-server>=1.0.0\n  Downloading jupyter_server-1.6.4-py3-none-any.whl (377 kB)\n\u001b[K     |████████████████████████████████| 377 kB 3.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from jupyter-resource-usage) (0.9.0)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (0.9.3)\nCollecting jupyter-packaging~=0.9\n  Downloading jupyter_packaging-0.9.2-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (0.2.0)\nCollecting anyio<3,>=2.0.2\n  Downloading anyio-2.2.0-py3-none-any.whl (65 kB)\n\u001b[K     |████████████████████████████████| 65 kB 3.2 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: tornado>=6.1.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (6.1)\nRequirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (6.0.7)\nRequirement already satisfied: jupyter-core>=4.4.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (4.7.1)\nRequirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (1.5.0)\nRequirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (22.0.3)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (20.1.0)\nRequirement already satisfied: traitlets>=4.2.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (5.0.5)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (2.11.3)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (5.1.2)\nRequirement already satisfied: jupyter-client>=6.1.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-server>=1.0.0->jupyter-resource-usage) (6.1.12)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.7/site-packages (from anyio<3,>=2.0.2->jupyter-server>=1.0.0->jupyter-resource-usage) (2.10)\nCollecting sniffio>=1.1\n  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from anyio<3,>=2.0.2->jupyter-server>=1.0.0->jupyter-resource-usage) (3.7.4.3)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.1->jupyter-server>=1.0.0->jupyter-resource-usage) (2.8.1)\nCollecting deprecation\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nCollecting tomlkit\n  Downloading tomlkit-0.7.0-py2.py3-none-any.whl (32 kB)\nRequirement already satisfied: setuptools>=46.4.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-packaging~=0.9->jupyter-server>=1.0.0->jupyter-resource-usage) (49.6.0.post20210108)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from jupyter-packaging~=0.9->jupyter-server>=1.0.0->jupyter-resource-usage) (20.9)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from jupyter-packaging~=0.9->jupyter-server>=1.0.0->jupyter-resource-usage) (0.36.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.1->jupyter-server>=1.0.0->jupyter-resource-usage) (1.15.0)\nRequirement already satisfied: ptyprocess in /opt/conda/lib/python3.7/site-packages (from terminado>=0.8.3->jupyter-server>=1.0.0->jupyter-resource-usage) (0.7.0)\nRequirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->jupyter-server>=1.0.0->jupyter-resource-usage) (1.14.5)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server>=1.0.0->jupyter-resource-usage) (2.20)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->jupyter-server>=1.0.0->jupyter-resource-usage) (1.1.1)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (1.4.2)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.4.4)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (3.3.0)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.1.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.5.3)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.8.4)\nRequirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.3)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.7.1)\nRequirement already satisfied: pygments>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (2.8.1)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (1.4.3)\nRequirement already satisfied: async-generator in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (1.10)\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (3.2.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (3.4.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (0.17.3)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (20.3.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->jupyter-server>=1.0.0->jupyter-resource-usage) (0.5.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->jupyter-server>=1.0.0->jupyter-resource-usage) (3.4.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->jupyter-packaging~=0.9->jupyter-server>=1.0.0->jupyter-resource-usage) (2.4.7)\nInstalling collected packages: tomlkit, sniffio, deprecation, jupyter-packaging, anyio, jupyter-server, jupyter-resource-usage\nSuccessfully installed anyio-2.2.0 deprecation-2.1.0 jupyter-packaging-0.9.2 jupyter-resource-usage-0.6.0 jupyter-server-1.6.4 sniffio-1.2.0 tomlkit-0.7.0\nCollecting mxnet\n  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n\u001b[K     |████████████████████████████████| 46.9 MB 55.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (1.19.5)\nRequirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from mxnet) (0.8.4)\nRequirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (2.25.1)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.10)\nInstalling collected packages: mxnet\nSuccessfully installed mxnet-1.8.0.post0\nRequirement already satisfied: gluonnlp in /opt/conda/lib/python3.7/site-packages (0.10.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.1.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.59.0)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (1.19.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (20.9)\nRequirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (0.29.23)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->gluonnlp) (2.4.7)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\nCollecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-ql_rcvfv\n  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-ql_rcvfv\nBuilding wheels for collected packages: kobert\n  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=12704 sha256=8bb142905a5bf3c4ea5bd78dc5040ec04f9b7474f4753a6e9da9b557602f1e7e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0l1v49ay/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\nSuccessfully built kobert\nInstalling collected packages: kobert\nSuccessfully installed kobert-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/yunakim2/OpenSourceProject/develop/data/labeled/samsung_2010_2021_01.csv\n\nimport gc\nimport torch\n\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup\nimport gluonnlp as nlp\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom kobert.utils import get_tokenizer\nfrom kobert.pytorch_kobert import get_pytorch_kobert_model\n\nimport numpy as np\nimport random\nimport time\nimport datetime\n\nimport io\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2021-05-04 14:54:02--  https://raw.githubusercontent.com/yunakim2/OpenSourceProject/develop/data/labeled/samsung_2010_2021_01.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 41356492 (39M) [text/plain]\nSaving to: ‘samsung_2010_2021_01.csv’\n\nsamsung_2010_2021_0 100%[===================>]  39.44M  77.9MB/s    in 0.5s    \n\n2021-05-04 14:54:03 (77.9 MB/s) - ‘samsung_2010_2021_01.csv’ saved [41356492/41356492]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"USE_CUDA = True\nRANDOM_SEED=43 # 재현을 위해 랜덤시드 고정\nTOKEN_MAX_LEN = 128*4\nBATCH_SIZE = 8 #로컬 6GB일때 1, 클라우드T4 16GB일때 12\nSTATUS_PRINT_INTERVAL=25\n\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\n\n# 정확도 계산 함수\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round(elapsed))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\nif USE_CUDA and torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print('No GPU available, using the CPU instead.')\n\nbertmodel, vocab = get_pytorch_kobert_model()\n\n#%%\n","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\nusing cached model\nusing cached model\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('samsung_2010_2021_01.csv',encoding = 'utf-8')\ntest_cnt = int(data.shape[0] * 0.25)\n\ntest = data[:test_cnt]\ntrain = data[test_cnt:]\n\ntok = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)\ntokenizer = nlp.data.BERTSentenceTransform(tok,max_seq_length=TOKEN_MAX_LEN,vocab=vocab,pair=False)\n\nprint('train&validation data processing')\ninput_ids = np.array([tokenizer([i])[0] for i in train['text']]).astype(int)\nprint(vocab.to_tokens([int(i) for i in input_ids[0][:20]]))\n\n''' 어텐션 마스크 '''\nattention_mask = []\nfor seq in input_ids:\n    seq_mask = [float(i > 0) for i in seq]\n    attention_mask.append(seq_mask)\n\n# print(attention_mask[0])\n\n'''train&test validation set 분리'''\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, train['label'].values, random_state=RANDOM_SEED, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_mask,input_ids,random_state=RANDOM_SEED,test_size=0.1)\n\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"using cached model\ntrain&validation data processing\n['[CLS]', '▁삼성', '▁', '로', '고', '▁자료', '사진', '▁W', 'S', 'J', '칼', '럼', '▁경쟁', '▁치열', '해', '▁장', '담', '▁못해', '▁모멘텀', '▁']\n","output_type":"stream"}]},{"cell_type":"code","source":"print('test data processing')\ninput_ids = np.array([tokenizer([i])[0] for i in test['text']]).astype(int)\nprint(vocab.to_tokens([int(i) for i in input_ids[0][:20]]))\n\n\nattention_masks = []\nfor seq in input_ids:\n    seq_mask = [float(i > 0) for i in seq]\n    attention_masks.append(seq_mask)\n\ntest_inputs = torch.tensor(input_ids)\ntest_labels = torch.tensor(test['label'].values)\ntest_masks = torch.tensor(attention_masks)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"test data processing\n['[CLS]', '▁올해', '▁100', '년', '▁기업', '을', '▁향한', '▁비', '전', '20', '20', '▁', '을', '▁구', '체', '화', '하기', '▁위해', '▁초', '경쟁력']\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"분류를 위한 BERT 모델 생성\"\"\"\n\nprint('분류를 위한 BERT 모델 생성 작업 중 - ')\nmodel = BertForSequenceClassification.from_pretrained(bertmodel.config.name_or_path, num_labels=2)\nmodel.double()\n\n\"\"\"학습 스케줄링\"\"\"\nprint('학습 스케줄링 중 - ')\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nepochs = 4\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"분류를 위한 BERT 모델 생성 작업 중 - \n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/kobert/kobert_from_pretrained and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"학습 스케줄링 중 - \n","output_type":"stream"}]},{"cell_type":"code","source":"# 재현을 위해 랜덤시드 고정\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# 그래디언트 초기화\nmodel.zero_grad()\ndevice = \"cuda:0\"\nmodel = model.to(device)\n\n# 에폭만큼 반복\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # 시작 시간 설정\n    t0 = time.time()\n\n    # 로스 초기화\n    total_loss = 0\n\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n\n    # 훈련모드로 변경\n    model.train()\n        \n    # 데이터로더에서 배치만큼 반복하여 가져옴\n    for step, batch in enumerate(train_dataloader):\n        # 경과 정보 표시\n        if step % 500 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n        \n\n        # 배치를 GPU에 넣음\n        batch = tuple(t.to(device) for t in batch)\n          # 배치에서 데이터 추출\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Forward 수행                \n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n\n        \n        # 로스 구함\n        loss = outputs[0]\n\n        # 총 로스 계산\n        total_loss += loss.item()\n\n        # Backward 수행으로 그래디언트 계산\n        loss.backward()\n\n        # 그래디언트 클리핑\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # 그래디언트를 통해 가중치 파라미터 업데이트\n        optimizer.step()\n\n        # 스케줄러로 학습률 감소\n        scheduler.step()\n\n        # 그래디언트 초기화\n        model.zero_grad()\n\n    # 평균 로스 계산\n    avg_train_loss = total_loss / len(train_dataloader)  \n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    #시작 시간 설정\n    t0 = time.time()\n\n     # 평가모드로 변경\n    model.eval()\n\n    # 변수 초기화\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # 데이터로더에서 배치만큼 반복하여 가져옴\n    for batch in validation_dataloader:\n        # 배치를 GPU에 넣음\n        batch = tuple(t.to(device) for t in batch)\n        \n        # 배치에서 데이터 추출\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # 그래디언트 계산 안함\n        with torch.no_grad():     \n            # Forward 수행\n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # 로스 구함\n        logits = outputs[0]\n\n        # CPU로 데이터 이동\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # 출력 로짓과 라벨을 비교하여 정확도 계산\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n\nprint(\"\")\nprint(\"Training complete!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 4 ========\nTraining...\n  Batch   500  of  1,042.    Elapsed: 0:06:32.\n  Batch 1,000  of  1,042.    Elapsed: 0:13:03.\n\n  Average training loss: 0.69\n  Training epcoh took: 0:13:35\n\nRunning Validation...\n  Accuracy: 0.55\n  Validation took: 0:00:28\n\n======== Epoch 2 / 4 ========\nTraining...\n  Batch   500  of  1,042.    Elapsed: 0:06:32.\n","output_type":"stream"}]},{"cell_type":"code","source":"# 시작 시간 설정\nt0 = time.time()\n\n# 평가모드로 변경\nmodel.eval()\n\n# 변수 초기화\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\n# 데이터로더에서 배치만큼 반복하여 가져옴\nfor step, batch in enumerate(test_dataloader):\n    # 경과 정보 표시\n    if step % 100 == 0 and not step == 0:\n        elapsed = format_time(time.time() - t0)\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n\n    # 배치를 GPU에 넣음\n    batch = tuple(t.to(device) for t in batch)\n    print(batch)\n    # 배치에서 데이터 추출\n    b_input_ids, b_input_mask, b_labels = batch\n\n    # 그래디언트 계산 안함\n    with torch.no_grad():\n        # Forward 수행\n        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask)\n\n    # 로스 구함\n    logits = outputs[0]\n\n    # CPU로 데이터 이동\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    # 출력 로짓과 라벨을 비교하여 정확도 계산\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\nprint(\"\")\nprint(\"Accuracy: {0:.2f}\".format(eval_accuracy / nb_eval_steps))\nprint(\"Test took: {:}\".format(format_time(time.time() - t0)))","metadata":{"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-263e0099fff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         outputs = model(b_input_ids,\n\u001b[1;32m     28\u001b[0m                         \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                         attention_mask=b_input_mask)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# 로스 구함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m         )\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m         )\n\u001b[1;32m    971\u001b[0m         encoder_outputs = self.encoder(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"],"ename":"RuntimeError","evalue":"Input, output and indices must be on the current device","output_type":"error"}]}]}